{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2024/06/finetuning-llama-3-for-sequence-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (4.11.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2024.6.2)\n",
      "Requirement already satisfied: wcwidth in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate trl bitsandbytes datasets evaluate\n",
    "!pip install -q peft scikit-learn\n",
    "!pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../private_/hf_read_token\", \"r\") as f:\n",
    "  token = f.readline()\n",
    "\n",
    "hf_read = token\n",
    "\n",
    "with open(\"../private_/hf_write_token\", \"r\") as f:\n",
    "  token = f.readline()\n",
    "\n",
    "hf_write = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/arjunsohur/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $hf_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/arjunsohur/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# not sure if the r and w tokens are needed but oh well\n",
    "login(token=hf_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "corpus = Corpus(filename=download(\"winning-args-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of ids 293297\n",
      "8106\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6484\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 811\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6484\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ids = corpus.get_utterance_ids()\n",
    "print(\"Len of ids\", len(ids))\n",
    "\n",
    "SPEAKER_BLACKLIST = ['DeltaBot','AutoModerator']\n",
    "training_trios = []\n",
    "\n",
    "for id in ids:\n",
    "  ut = corpus.get_utterance(id)\n",
    "  if ut.reply_to == ut.conversation_id and (ut.meta['success'] == 1 or ut.meta['success'] == 0) and (ut.speaker.id not in SPEAKER_BLACKLIST):\n",
    "    op = corpus.get_utterance(ut.conversation_id).text\n",
    "    x = ut.text\n",
    "    y = ut.meta['success']\n",
    "\n",
    "    training_trios += [(op, x, y)]\n",
    "\n",
    "print(len(training_trios))\n",
    "\n",
    "train_len = len(training_trios)\n",
    "\n",
    "ones = 0\n",
    "zeros = 0\n",
    "total = 0\n",
    "\n",
    "def formatting_prompts_func(training_trios):\n",
    "    texts = []\n",
    "    targets = []\n",
    "\n",
    "    total = 0\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "\n",
    "    for trio in training_trios:\n",
    "        op, x, y = trio\n",
    "        instruction = \"Please determine if the following argument is successful based on the original post.  Output 1 for successful and 0 for unsuccessful.  Only output the one number, NOTHING ELSE.\"\n",
    "        input_context = f\"Original post: {op}\\nArgument: {x}\"\n",
    "\n",
    "        texts.append(input_context)\n",
    "        targets.append(y)\n",
    "\n",
    "        if y:\n",
    "           ones+=1\n",
    "        else:\n",
    "           zeros+=1\n",
    "        total += 1\n",
    "\n",
    "    return texts, targets, ones, zeros, total\n",
    "\n",
    "# Format the data\n",
    "texts, targets, ones, zeros, total = formatting_prompts_func(training_trios)\n",
    "\n",
    "v_start = int(len(texts) * 0.8)\n",
    "v_end = int(len(texts) * 0.9)\n",
    "\n",
    "train = {\"text\": texts[:v_start], \"label\": targets[:v_start]}\n",
    "val = {\"text\": texts[v_start:v_end], \"label\":targets[v_start:v_end]}\n",
    "test = {\"text\": texts[v_end:], \"label\":targets[v_end:]}\n",
    "\n",
    "train_ds = Dataset.from_dict(train)\n",
    "val_ds = Dataset.from_dict(val)\n",
    "test_ds = Dataset.from_dict(test)\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train_ds)\n",
    "val_df = pd.DataFrame.from_dict(val_ds)\n",
    "test_df = pd.DataFrame.from_dict(test_ds)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "   'train': train_ds,\n",
    "   'val': val_ds,\n",
    "   'test': train_ds\n",
    "})\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "# NOTE: I DON'T WEIGHT THE 0 OR 1, WHICH I SHOULD PROBABLY DO\n",
    "# IF PERFORMACE IS UNSATISFACTORY, I'LL DO IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5216, 0.4784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights=(1/train_df.label.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa6d404953684986bf4cfd625dde33e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=2,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = test_df.text.tolist()\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", \n",
    "    padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        all_outputs.append(outputs['logits'])\n",
    "        \n",
    "final_outputs = torch.cat(all_outputs, dim=0)\n",
    "test_df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.68      0.56       382\n",
      "           1       0.56      0.36      0.44       429\n",
      "\n",
      "    accuracy                           0.51       811\n",
      "   macro avg       0.52      0.52      0.50       811\n",
      "weighted avg       0.52      0.51      0.50       811\n",
      "\n",
      "Balanced Accuracy Score: 0.5183490157312147\n",
      "Accuracy Score: 0.5092478421701603\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "def get_metrics_result(test_df):\n",
    "    y_test = test_df.label\n",
    "    y_pred = test_df.predictions\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "get_metrics_result(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de948fb2f94b43a3837e20c47df013ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da888e0675142f0af387ceb9246a643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9890eda9f4314c74bfdcd4a48c4cedea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_preprocesing(row):\n",
    "    return tokenizer(row['text'], truncation=True, max_length=1000)\n",
    "\n",
    "tokenized_data = dataset.map(data_preprocesing, batched=True, \n",
    "remove_columns=['text'])\n",
    "tokenized_data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(evaluations):\n",
    "    predictions, labels = evaluations\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),\n",
    "    'accuracy':accuracy_score(predictions,labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, \n",
    "            dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='persuasion_classification',\n",
    "    learning_rate=2e-5,  # Start with a lower learning rate\n",
    "    per_device_train_batch_size=8,  # You can experiment with 16 or 32 as well\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,  # Increase the number of epochs\n",
    "    logging_steps=10,  # Adjust logging steps for less frequent logging\n",
    "    weight_decay=0.05,  # Try a higher weight decay\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type='linear',  # Use a learning rate scheduler\n",
    "    warmup_steps=500  # Add warmup steps for the scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_2238300/1916366495.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights,\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data['train'],\n",
    "    eval_dataset = tokenized_data['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8' max='2433' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   8/2433 01:13 < 8:15:15, 0.08 it/s, Epoch 0.01/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_result \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/transformers/trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1886\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1887\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1888\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1889\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1890\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/transformers/trainer.py:2221\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2222\u001b[0m ):\n\u001b[1;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(model,df_test):\n",
    "    sentences = df_test.text.tolist()\n",
    "    batch_size = 32  \n",
    "    all_outputs = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", \n",
    "        padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "        for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "        \n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "\n",
    "generate_predictions(model,test_df)\n",
    "get_metrics_result(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"ArjunSohur/argument_classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persuasion_env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
