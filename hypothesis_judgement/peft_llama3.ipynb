{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2024/06/finetuning-llama-3-for-sequence-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[cli] in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (0.23.4)\n",
      "Requirement already satisfied: filelock in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (2024.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (4.11.0)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.43)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2024.6.2)\n",
      "Requirement already satisfied: wcwidth in /home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers accelerate trl bitsandbytes datasets evaluate\n",
    "!pip install -q peft scikit-learn\n",
    "!pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../private_/hf_read_token\", \"r\") as f:\n",
    "  token = f.readline()\n",
    "\n",
    "hf_read = token\n",
    "\n",
    "with open(\"../private_/hf_write_token\", \"r\") as f:\n",
    "  token = f.readline()\n",
    "\n",
    "hf_write = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/arjunsohur/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $hf_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/arjunsohur/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# not sure if the r and w tokens are needed but oh well\n",
    "login(token=hf_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "corpus = Corpus(filename=download(\"winning-args-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of ids 293297\n",
      "8106\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6484\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 811\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 6484\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ids = corpus.get_utterance_ids()\n",
    "print(\"Len of ids\", len(ids))\n",
    "\n",
    "SPEAKER_BLACKLIST = ['DeltaBot','AutoModerator']\n",
    "training_trios = []\n",
    "\n",
    "for id in ids:\n",
    "  ut = corpus.get_utterance(id)\n",
    "  if ut.reply_to == ut.conversation_id and (ut.meta['success'] == 1 or ut.meta['success'] == 0) and (ut.speaker.id not in SPEAKER_BLACKLIST):\n",
    "    op = corpus.get_utterance(ut.conversation_id).text\n",
    "    x = ut.text\n",
    "    y = ut.meta['success']\n",
    "\n",
    "    training_trios += [(op, x, y)]\n",
    "\n",
    "print(len(training_trios))\n",
    "\n",
    "train_len = len(training_trios)\n",
    "\n",
    "ones = 0\n",
    "zeros = 0\n",
    "total = 0\n",
    "\n",
    "def formatting_prompts_func(training_trios):\n",
    "    texts = []\n",
    "    targets = []\n",
    "\n",
    "    total = 0\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "\n",
    "    for trio in training_trios:\n",
    "        op, x, y = trio\n",
    "        instruction = \"Please determine if the following argument is successful based on the original post.  Output 1 for successful and 0 for unsuccessful.  Only output the one number, NOTHING ELSE.\"\n",
    "        input_context = f\"Original post: {op}\\nArgument: {x}\"\n",
    "\n",
    "        texts.append(input_context)\n",
    "        targets.append(y)\n",
    "\n",
    "        if y:\n",
    "           ones+=1\n",
    "        else:\n",
    "           zeros+=1\n",
    "        total += 1\n",
    "\n",
    "    return texts, targets, ones, zeros, total\n",
    "\n",
    "# Format the data\n",
    "texts, targets, ones, zeros, total = formatting_prompts_func(training_trios)\n",
    "\n",
    "v_start = int(len(texts) * 0.8)\n",
    "v_end = int(len(texts) * 0.9)\n",
    "\n",
    "train = {\"text\": texts[:v_start], \"label\": targets[:v_start]}\n",
    "val = {\"text\": texts[v_start:v_end], \"label\":targets[v_start:v_end]}\n",
    "test = {\"text\": texts[v_end:], \"label\":targets[v_end:]}\n",
    "\n",
    "train_ds = Dataset.from_dict(train)\n",
    "val_ds = Dataset.from_dict(val)\n",
    "test_ds = Dataset.from_dict(test)\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train_ds)\n",
    "val_df = pd.DataFrame.from_dict(val_ds)\n",
    "test_df = pd.DataFrame.from_dict(test_ds)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "   'train': train_ds,\n",
    "   'val': val_ds,\n",
    "   'test': train_ds\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5216, 0.4784])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights=(1/train_df.label.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae9f73491ac4a4f8d57fc3e3497b7b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=2,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = test_df.text.tolist()\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", \n",
    "    padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        all_outputs.append(outputs['logits'])\n",
    "        \n",
    "final_outputs = torch.cat(all_outputs, dim=0)\n",
    "test_df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.73      0.57       382\n",
      "           1       0.52      0.26      0.35       429\n",
      "\n",
      "    accuracy                           0.48       811\n",
      "   macro avg       0.49      0.50      0.46       811\n",
      "weighted avg       0.50      0.48      0.45       811\n",
      "\n",
      "Balanced Accuracy Score: 0.4955759772513699\n",
      "Accuracy Score: 0.48212083847102344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "def get_metrics_result(test_df):\n",
    "    y_test = test_df.label\n",
    "    y_pred = test_df.predictions\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "get_metrics_result(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a406f9d580f449cbbdb8c9856ab1b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80de592431734757a604e988ba33ae32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/811 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91eadcb7c79e46ed884ec3be0285e70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6484 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_preprocesing(row):\n",
    "    return tokenizer(row['text'], truncation=True, max_length=1000)\n",
    "\n",
    "tokenized_data = dataset.map(data_preprocesing, batched=True, \n",
    "remove_columns=['text'])\n",
    "tokenized_data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(evaluations):\n",
    "    predictions, labels = evaluations\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),\n",
    "    'accuracy':accuracy_score(predictions,labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        print(f\"LOSS: {loss}\")\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='persuasion_classification',\n",
    "    learning_rate=2e-5,  \n",
    "    per_device_train_batch_size=8, \n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1, \n",
    "    logging_steps=10,  \n",
    "    weight_decay=0.05, \n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type='linear', \n",
    "    warmup_steps=500 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "/tmp/ipykernel_3632641/2509682323.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_data['train'],\n",
    "    eval_dataset = tokenized_data['val'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 2.9285895824432373\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='811' max='811' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [811/811 5:27:53, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.187000</td>\n",
       "      <td>1.014432</td>\n",
       "      <td>0.519871</td>\n",
       "      <td>0.522811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 3.233867883682251\n",
      "LOSS: 0.4995625913143158\n",
      "LOSS: 1.123119831085205\n",
      "LOSS: 0.8993872404098511\n",
      "LOSS: 2.8484628200531006\n",
      "LOSS: 1.0482702255249023\n",
      "LOSS: 1.8126550912857056\n",
      "LOSS: 2.1227669715881348\n",
      "LOSS: 1.396763563156128\n",
      "LOSS: 3.729778289794922\n",
      "LOSS: 2.0314369201660156\n",
      "LOSS: 0.8781561255455017\n",
      "LOSS: 2.3993725776672363\n",
      "LOSS: 2.9005744457244873\n",
      "LOSS: 4.276153564453125\n",
      "LOSS: 2.63820219039917\n",
      "LOSS: 2.6977288722991943\n",
      "LOSS: 1.1917229890823364\n",
      "LOSS: 1.8824288845062256\n",
      "LOSS: 3.5396547317504883\n",
      "LOSS: 0.8359293341636658\n",
      "LOSS: 1.1620724201202393\n",
      "LOSS: 0.8840441703796387\n",
      "LOSS: 1.0595492124557495\n",
      "LOSS: 1.0707199573516846\n",
      "LOSS: 0.503533124923706\n",
      "LOSS: 4.280075550079346\n",
      "LOSS: 1.3797043561935425\n",
      "LOSS: 2.1827824115753174\n",
      "LOSS: 0.9695549011230469\n",
      "LOSS: 1.611816167831421\n",
      "LOSS: 2.428082227706909\n",
      "LOSS: 1.194408655166626\n",
      "LOSS: 2.0698471069335938\n",
      "LOSS: 0.9329428672790527\n",
      "LOSS: 1.0080509185791016\n",
      "LOSS: 2.522871255874634\n",
      "LOSS: 2.046497106552124\n",
      "LOSS: 0.8162161111831665\n",
      "LOSS: 0.7340665459632874\n",
      "LOSS: 2.291846990585327\n",
      "LOSS: 3.107546329498291\n",
      "LOSS: 1.6562976837158203\n",
      "LOSS: 2.522817373275757\n",
      "LOSS: 1.3064520359039307\n",
      "LOSS: 1.1192048788070679\n",
      "LOSS: 1.034993052482605\n",
      "LOSS: 0.5409640073776245\n",
      "LOSS: 2.1325442790985107\n",
      "LOSS: 3.0418541431427\n",
      "LOSS: 1.3723725080490112\n",
      "LOSS: 2.2042531967163086\n",
      "LOSS: 1.8481676578521729\n",
      "LOSS: 1.0678824186325073\n",
      "LOSS: 3.494703769683838\n",
      "LOSS: 1.6951593160629272\n",
      "LOSS: 2.4719583988189697\n",
      "LOSS: 0.8503870368003845\n",
      "LOSS: 1.588728904724121\n",
      "LOSS: 0.5070392489433289\n",
      "LOSS: 0.565572202205658\n",
      "LOSS: 0.3579329252243042\n",
      "LOSS: 2.3764946460723877\n",
      "LOSS: 1.4221220016479492\n",
      "LOSS: 1.1575024127960205\n",
      "LOSS: 2.362823724746704\n",
      "LOSS: 1.2726526260375977\n",
      "LOSS: 1.4719616174697876\n",
      "LOSS: 1.1748496294021606\n",
      "LOSS: 0.8963937759399414\n",
      "LOSS: 2.300248622894287\n",
      "LOSS: 2.7355029582977295\n",
      "LOSS: 1.7077915668487549\n",
      "LOSS: 0.7789124250411987\n",
      "LOSS: 2.132157802581787\n",
      "LOSS: 1.9975699186325073\n",
      "LOSS: 3.664238452911377\n",
      "LOSS: 1.2079517841339111\n",
      "LOSS: 1.2713615894317627\n",
      "LOSS: 4.079057216644287\n",
      "LOSS: 3.744940757751465\n",
      "LOSS: 3.0248241424560547\n",
      "LOSS: 0.9801003932952881\n",
      "LOSS: 1.1597744226455688\n",
      "LOSS: 2.2113001346588135\n",
      "LOSS: 0.7220394015312195\n",
      "LOSS: 1.7960643768310547\n",
      "LOSS: 1.4338752031326294\n",
      "LOSS: 0.69921875\n",
      "LOSS: 1.0997453927993774\n",
      "LOSS: 1.8102855682373047\n",
      "LOSS: 1.3198812007904053\n",
      "LOSS: 3.478311061859131\n",
      "LOSS: 1.337862491607666\n",
      "LOSS: 3.375509262084961\n",
      "LOSS: 2.044921875\n",
      "LOSS: 1.9286885261535645\n",
      "LOSS: 1.6791704893112183\n",
      "LOSS: 1.5688003301620483\n",
      "LOSS: 1.6745644807815552\n",
      "LOSS: 1.2980780601501465\n",
      "LOSS: 2.206976890563965\n",
      "LOSS: 0.6449862122535706\n",
      "LOSS: 1.4903956651687622\n",
      "LOSS: 1.370620608329773\n",
      "LOSS: 1.4239286184310913\n",
      "LOSS: 1.1483750343322754\n",
      "LOSS: 0.932009220123291\n",
      "LOSS: 2.414595603942871\n",
      "LOSS: 3.647855043411255\n",
      "LOSS: 3.0244789123535156\n",
      "LOSS: 1.47439706325531\n",
      "LOSS: 1.855623722076416\n",
      "LOSS: 2.454761505126953\n",
      "LOSS: 0.7402899861335754\n",
      "LOSS: 2.3641059398651123\n",
      "LOSS: 0.5615440011024475\n",
      "LOSS: 1.0793790817260742\n",
      "LOSS: 1.594864010810852\n",
      "LOSS: 1.2944587469100952\n",
      "LOSS: 1.0794731378555298\n",
      "LOSS: 1.8411295413970947\n",
      "LOSS: 0.9393980503082275\n",
      "LOSS: 2.394599199295044\n",
      "LOSS: 0.34784644842147827\n",
      "LOSS: 1.9542393684387207\n",
      "LOSS: 1.9150362014770508\n",
      "LOSS: 2.937168598175049\n",
      "LOSS: 1.063033938407898\n",
      "LOSS: 2.642015218734741\n",
      "LOSS: 1.953883409500122\n",
      "LOSS: 1.1879298686981201\n",
      "LOSS: 1.1531578302383423\n",
      "LOSS: 1.238353967666626\n",
      "LOSS: 0.8409979343414307\n",
      "LOSS: 2.992023468017578\n",
      "LOSS: 1.8320626020431519\n",
      "LOSS: 2.3766136169433594\n",
      "LOSS: 1.997748851776123\n",
      "LOSS: 1.7783260345458984\n",
      "LOSS: 2.280154228210449\n",
      "LOSS: 1.0830739736557007\n",
      "LOSS: 0.9548205137252808\n",
      "LOSS: 1.179166316986084\n",
      "LOSS: 1.713744878768921\n",
      "LOSS: 1.2474476099014282\n",
      "LOSS: 1.5945795774459839\n",
      "LOSS: 2.356654167175293\n",
      "LOSS: 0.4282483756542206\n",
      "LOSS: 1.7125847339630127\n",
      "LOSS: 1.792777180671692\n",
      "LOSS: 2.4482693672180176\n",
      "LOSS: 1.6068578958511353\n",
      "LOSS: 2.5298912525177\n",
      "LOSS: 2.5405993461608887\n",
      "LOSS: 0.35900285840034485\n",
      "LOSS: 1.7388830184936523\n",
      "LOSS: 1.3348491191864014\n",
      "LOSS: 2.0413897037506104\n",
      "LOSS: 1.345097303390503\n",
      "LOSS: 1.347493052482605\n",
      "LOSS: 3.0176868438720703\n",
      "LOSS: 2.0472145080566406\n",
      "LOSS: 1.8544410467147827\n",
      "LOSS: 1.5348377227783203\n",
      "LOSS: 1.304113507270813\n",
      "LOSS: 1.2021719217300415\n",
      "LOSS: 2.164266586303711\n",
      "LOSS: 1.3405572175979614\n",
      "LOSS: 1.2959166765213013\n",
      "LOSS: 1.6209512948989868\n",
      "LOSS: 1.3805108070373535\n",
      "LOSS: 1.0255917310714722\n",
      "LOSS: 0.9103988409042358\n",
      "LOSS: 2.211007595062256\n",
      "LOSS: 1.3558094501495361\n",
      "LOSS: 1.1857661008834839\n",
      "LOSS: 1.1204708814620972\n",
      "LOSS: 1.145188808441162\n",
      "LOSS: 1.174080729484558\n",
      "LOSS: 1.475874900817871\n",
      "LOSS: 0.5385246276855469\n",
      "LOSS: 2.913649559020996\n",
      "LOSS: 1.446540355682373\n",
      "LOSS: 1.7268140316009521\n",
      "LOSS: 2.1743741035461426\n",
      "LOSS: 1.2697792053222656\n",
      "LOSS: 3.125223159790039\n",
      "LOSS: 0.8387367129325867\n",
      "LOSS: 2.763166666030884\n",
      "LOSS: 4.218714237213135\n",
      "LOSS: 1.1431961059570312\n",
      "LOSS: 1.660616159439087\n",
      "LOSS: 0.8239744901657104\n",
      "LOSS: 0.8032503724098206\n",
      "LOSS: 1.2866545915603638\n",
      "LOSS: 1.7154934406280518\n",
      "LOSS: 0.7435646653175354\n",
      "LOSS: 1.334471344947815\n",
      "LOSS: 1.2845507860183716\n",
      "LOSS: 1.561417579650879\n",
      "LOSS: 1.4759535789489746\n",
      "LOSS: 1.8728413581848145\n",
      "LOSS: 1.242287278175354\n",
      "LOSS: 0.3791913688182831\n",
      "LOSS: 1.5568645000457764\n",
      "LOSS: 2.349914073944092\n",
      "LOSS: 2.0401852130889893\n",
      "LOSS: 1.0795735120773315\n",
      "LOSS: 2.289735794067383\n",
      "LOSS: 3.6092090606689453\n",
      "LOSS: 2.0596022605895996\n",
      "LOSS: 1.2367560863494873\n",
      "LOSS: 1.0420682430267334\n",
      "LOSS: 1.7790318727493286\n",
      "LOSS: 1.1236313581466675\n",
      "LOSS: 1.5861337184906006\n",
      "LOSS: 1.406447410583496\n",
      "LOSS: 2.188849449157715\n",
      "LOSS: 2.3037188053131104\n",
      "LOSS: 0.9458221793174744\n",
      "LOSS: 2.1961212158203125\n",
      "LOSS: 0.5951505899429321\n",
      "LOSS: 1.007964015007019\n",
      "LOSS: 1.9781938791275024\n",
      "LOSS: 1.7432827949523926\n",
      "LOSS: 0.6914148330688477\n",
      "LOSS: 0.6607533693313599\n",
      "LOSS: 1.702894926071167\n",
      "LOSS: 0.6614029407501221\n",
      "LOSS: 1.1865553855895996\n",
      "LOSS: 0.7638863921165466\n",
      "LOSS: 0.8929722309112549\n",
      "LOSS: 0.9411867260932922\n",
      "LOSS: 1.224414348602295\n",
      "LOSS: 1.6333794593811035\n",
      "LOSS: 0.8960127830505371\n",
      "LOSS: 1.2498180866241455\n",
      "LOSS: 1.0991350412368774\n",
      "LOSS: 2.368617534637451\n",
      "LOSS: 1.0027830600738525\n",
      "LOSS: 1.2192883491516113\n",
      "LOSS: 0.5700212121009827\n",
      "LOSS: 1.6195523738861084\n",
      "LOSS: 1.3437567949295044\n",
      "LOSS: 0.7258464694023132\n",
      "LOSS: 2.0132951736450195\n",
      "LOSS: 1.6470284461975098\n",
      "LOSS: 1.4647061824798584\n",
      "LOSS: 1.0540382862091064\n",
      "LOSS: 2.063497543334961\n",
      "LOSS: 3.138387680053711\n",
      "LOSS: 1.5152137279510498\n",
      "LOSS: 1.5929315090179443\n",
      "LOSS: 0.4692714810371399\n",
      "LOSS: 0.9683874249458313\n",
      "LOSS: 1.0461201667785645\n",
      "LOSS: 0.44874995946884155\n",
      "LOSS: 1.178200602531433\n",
      "LOSS: 1.3163267374038696\n",
      "LOSS: 1.0901259183883667\n",
      "LOSS: 2.324162483215332\n",
      "LOSS: 1.833078384399414\n",
      "LOSS: 1.8990973234176636\n",
      "LOSS: 1.8176233768463135\n",
      "LOSS: 1.0345348119735718\n",
      "LOSS: 1.9306871891021729\n",
      "LOSS: 3.9159486293792725\n",
      "LOSS: 1.119551181793213\n",
      "LOSS: 1.0574967861175537\n",
      "LOSS: 0.7706279754638672\n",
      "LOSS: 2.1200129985809326\n",
      "LOSS: 2.140455722808838\n",
      "LOSS: 3.089937925338745\n",
      "LOSS: 0.6573887467384338\n",
      "LOSS: 1.358367919921875\n",
      "LOSS: 1.489867925643921\n",
      "LOSS: 0.6829025149345398\n",
      "LOSS: 0.9443894624710083\n",
      "LOSS: 1.3783900737762451\n",
      "LOSS: 1.2091344594955444\n",
      "LOSS: 1.1733225584030151\n",
      "LOSS: 0.9664139151573181\n",
      "LOSS: 0.9290761947631836\n",
      "LOSS: 0.8212448954582214\n",
      "LOSS: 1.2062479257583618\n",
      "LOSS: 2.9249353408813477\n",
      "LOSS: 0.978858470916748\n",
      "LOSS: 0.9652304649353027\n",
      "LOSS: 1.4079359769821167\n",
      "LOSS: 1.909263253211975\n",
      "LOSS: 0.5979433655738831\n",
      "LOSS: 1.8267970085144043\n",
      "LOSS: 1.0192930698394775\n",
      "LOSS: 0.7161777019500732\n",
      "LOSS: 2.6162214279174805\n",
      "LOSS: 0.34753096103668213\n",
      "LOSS: 1.7489206790924072\n",
      "LOSS: 1.4114151000976562\n",
      "LOSS: 1.6998275518417358\n",
      "LOSS: 1.2905024290084839\n",
      "LOSS: 0.7162894606590271\n",
      "LOSS: 1.6722222566604614\n",
      "LOSS: 1.6529868841171265\n",
      "LOSS: 1.0414011478424072\n",
      "LOSS: 1.248199224472046\n",
      "LOSS: 1.4602000713348389\n",
      "LOSS: 1.3779125213623047\n",
      "LOSS: 1.207102656364441\n",
      "LOSS: 1.776078462600708\n",
      "LOSS: 2.6653554439544678\n",
      "LOSS: 1.1569833755493164\n",
      "LOSS: 0.55510014295578\n",
      "LOSS: 2.3159847259521484\n",
      "LOSS: 1.0196419954299927\n",
      "LOSS: 1.1543490886688232\n",
      "LOSS: 0.655541718006134\n",
      "LOSS: 1.5999481678009033\n",
      "LOSS: 0.6701300144195557\n",
      "LOSS: 1.061290979385376\n",
      "LOSS: 1.4508659839630127\n",
      "LOSS: 2.1778149604797363\n",
      "LOSS: 0.7070989012718201\n",
      "LOSS: 1.0046818256378174\n",
      "LOSS: 1.2374732494354248\n",
      "LOSS: 1.2500938177108765\n",
      "LOSS: 0.8490819334983826\n",
      "LOSS: 0.45735976099967957\n",
      "LOSS: 2.167161703109741\n",
      "LOSS: 1.160385012626648\n",
      "LOSS: 0.8041077852249146\n",
      "LOSS: 1.5647821426391602\n",
      "LOSS: 1.0690444707870483\n",
      "LOSS: 0.4246233105659485\n",
      "LOSS: 1.1475602388381958\n",
      "LOSS: 2.0379014015197754\n",
      "LOSS: 1.0701433420181274\n",
      "LOSS: 1.0699927806854248\n",
      "LOSS: 0.6656317710876465\n",
      "LOSS: 1.071846604347229\n",
      "LOSS: 0.620668351650238\n",
      "LOSS: 1.4022244215011597\n",
      "LOSS: 1.779954433441162\n",
      "LOSS: 1.6109285354614258\n",
      "LOSS: 1.5674699544906616\n",
      "LOSS: 2.01515793800354\n",
      "LOSS: 1.3415597677230835\n",
      "LOSS: 0.7827286124229431\n",
      "LOSS: 0.6877942085266113\n",
      "LOSS: 1.1377177238464355\n",
      "LOSS: 0.4124578833580017\n",
      "LOSS: 1.6663799285888672\n",
      "LOSS: 0.4357086420059204\n",
      "LOSS: 2.0704305171966553\n",
      "LOSS: 0.8137778043746948\n",
      "LOSS: 0.6626562476158142\n",
      "LOSS: 1.1702042818069458\n",
      "LOSS: 0.8363931179046631\n",
      "LOSS: 0.5972414612770081\n",
      "LOSS: 1.4713597297668457\n",
      "LOSS: 0.5365279316902161\n",
      "LOSS: 1.7904688119888306\n",
      "LOSS: 0.30679261684417725\n",
      "LOSS: 0.9288700222969055\n",
      "LOSS: 0.6849543452262878\n",
      "LOSS: 1.821268916130066\n",
      "LOSS: 1.1236854791641235\n",
      "LOSS: 1.0137470960617065\n",
      "LOSS: 1.3851374387741089\n",
      "LOSS: 0.6104193329811096\n",
      "LOSS: 0.8616045117378235\n",
      "LOSS: 0.7170997858047485\n",
      "LOSS: 1.887010097503662\n",
      "LOSS: 2.4301340579986572\n",
      "LOSS: 0.41047608852386475\n",
      "LOSS: 1.3256354331970215\n",
      "LOSS: 1.0822111368179321\n",
      "LOSS: 1.2757338285446167\n",
      "LOSS: 0.963131844997406\n",
      "LOSS: 1.0438817739486694\n",
      "LOSS: 1.8108149766921997\n",
      "LOSS: 0.7795084118843079\n",
      "LOSS: 0.88127201795578\n",
      "LOSS: 1.129351019859314\n",
      "LOSS: 1.352277159690857\n",
      "LOSS: 1.4823235273361206\n",
      "LOSS: 1.5316673517227173\n",
      "LOSS: 0.8867817521095276\n",
      "LOSS: 1.2164634466171265\n",
      "LOSS: 0.3464091718196869\n",
      "LOSS: 0.8035290837287903\n",
      "LOSS: 1.037920355796814\n",
      "LOSS: 0.9956353306770325\n",
      "LOSS: 1.1626237630844116\n",
      "LOSS: 1.1724555492401123\n",
      "LOSS: 0.8583455085754395\n",
      "LOSS: 2.294139862060547\n",
      "LOSS: 1.4454489946365356\n",
      "LOSS: 0.5429483652114868\n",
      "LOSS: 0.5328819751739502\n",
      "LOSS: 1.0022025108337402\n",
      "LOSS: 0.7135540246963501\n",
      "LOSS: 1.600238561630249\n",
      "LOSS: 1.339665412902832\n",
      "LOSS: 1.6105215549468994\n",
      "LOSS: 1.969943881034851\n",
      "LOSS: 2.3514833450317383\n",
      "LOSS: 1.221397042274475\n",
      "LOSS: 0.7926594018936157\n",
      "LOSS: 1.2574306726455688\n",
      "LOSS: 1.0934206247329712\n",
      "LOSS: 0.7725136280059814\n",
      "LOSS: 1.0995455980300903\n",
      "LOSS: 1.0567290782928467\n",
      "LOSS: 1.344523549079895\n",
      "LOSS: 1.6825059652328491\n",
      "LOSS: 0.5106498599052429\n",
      "LOSS: 0.860689103603363\n",
      "LOSS: 0.7820079326629639\n",
      "LOSS: 1.4490374326705933\n",
      "LOSS: 1.765202283859253\n",
      "LOSS: 1.3042789697647095\n",
      "LOSS: 0.2859210968017578\n",
      "LOSS: 0.901667594909668\n",
      "LOSS: 1.354804515838623\n",
      "LOSS: 0.9336379766464233\n",
      "LOSS: 1.2654918432235718\n",
      "LOSS: 1.467611312866211\n",
      "LOSS: 1.2277510166168213\n",
      "LOSS: 1.1678295135498047\n",
      "LOSS: 1.799523949623108\n",
      "LOSS: 1.5651122331619263\n",
      "LOSS: 1.1466398239135742\n",
      "LOSS: 0.8092315793037415\n",
      "LOSS: 0.6136499047279358\n",
      "LOSS: 0.9317132234573364\n",
      "LOSS: 0.7104715704917908\n",
      "LOSS: 1.4017410278320312\n",
      "LOSS: 1.7786344289779663\n",
      "LOSS: 0.5882611870765686\n",
      "LOSS: 0.5107357501983643\n",
      "LOSS: 1.6118276119232178\n",
      "LOSS: 0.7588193416595459\n",
      "LOSS: 1.9515217542648315\n",
      "LOSS: 1.0627714395523071\n",
      "LOSS: 0.9940764307975769\n",
      "LOSS: 1.052531123161316\n",
      "LOSS: 1.0247055292129517\n",
      "LOSS: 1.8608235120773315\n",
      "LOSS: 1.5889302492141724\n",
      "LOSS: 1.1891813278198242\n",
      "LOSS: 1.6399586200714111\n",
      "LOSS: 0.6337350606918335\n",
      "LOSS: 1.1570106744766235\n",
      "LOSS: 1.0686091184616089\n",
      "LOSS: 0.8490347266197205\n",
      "LOSS: 0.34819215536117554\n",
      "LOSS: 1.5635180473327637\n",
      "LOSS: 0.6643989086151123\n",
      "LOSS: 0.8183789253234863\n",
      "LOSS: 0.84751957654953\n",
      "LOSS: 1.694153904914856\n",
      "LOSS: 1.169566035270691\n",
      "LOSS: 0.49757757782936096\n",
      "LOSS: 1.362012505531311\n",
      "LOSS: 1.358855962753296\n",
      "LOSS: 1.2601432800292969\n",
      "LOSS: 1.2715431451797485\n",
      "LOSS: 0.9274356365203857\n",
      "LOSS: 0.8893346786499023\n",
      "LOSS: 1.005946397781372\n",
      "LOSS: 0.7046071887016296\n",
      "LOSS: 0.715042769908905\n",
      "LOSS: 1.2967889308929443\n",
      "LOSS: 1.6151254177093506\n",
      "LOSS: 0.7268718481063843\n",
      "LOSS: 1.0782504081726074\n",
      "LOSS: 1.4609637260437012\n",
      "LOSS: 0.3234826624393463\n",
      "LOSS: 1.2692346572875977\n",
      "LOSS: 1.440222144126892\n",
      "LOSS: 0.8656688928604126\n",
      "LOSS: 1.8083029985427856\n",
      "LOSS: 0.7551292181015015\n",
      "LOSS: 1.318051815032959\n",
      "LOSS: 1.0281494855880737\n",
      "LOSS: 2.155952215194702\n",
      "LOSS: 1.0715553760528564\n",
      "LOSS: 1.516291618347168\n",
      "LOSS: 1.9989656209945679\n",
      "LOSS: 1.6538844108581543\n",
      "LOSS: 0.8065659403800964\n",
      "LOSS: 0.8568325042724609\n",
      "LOSS: 0.7332815527915955\n",
      "LOSS: 0.6968767642974854\n",
      "LOSS: 1.5317763090133667\n",
      "LOSS: 0.8961182832717896\n",
      "LOSS: 0.5314376950263977\n",
      "LOSS: 1.835436463356018\n",
      "LOSS: 1.4622485637664795\n",
      "LOSS: 1.1316112279891968\n",
      "LOSS: 1.110608696937561\n",
      "LOSS: 1.3341165781021118\n",
      "LOSS: 0.5274984836578369\n",
      "LOSS: 0.7749448418617249\n",
      "LOSS: 2.1489648818969727\n",
      "LOSS: 0.9988299608230591\n",
      "LOSS: 0.4693998694419861\n",
      "LOSS: 1.277316927909851\n",
      "LOSS: 1.4242430925369263\n",
      "LOSS: 1.4911812543869019\n",
      "LOSS: 1.0785284042358398\n",
      "LOSS: 1.1470531225204468\n",
      "LOSS: 1.4106953144073486\n",
      "LOSS: 0.5285281538963318\n",
      "LOSS: 0.8507068157196045\n",
      "LOSS: 0.7306002378463745\n",
      "LOSS: 1.1684813499450684\n",
      "LOSS: 0.9485427737236023\n",
      "LOSS: 2.0953948497772217\n",
      "LOSS: 0.8421132564544678\n",
      "LOSS: 1.4237046241760254\n",
      "LOSS: 1.4542473554611206\n",
      "LOSS: 0.8029084801673889\n",
      "LOSS: 1.0474494695663452\n",
      "LOSS: 0.8226842284202576\n",
      "LOSS: 0.9365076422691345\n",
      "LOSS: 0.8282191753387451\n",
      "LOSS: 0.7192052602767944\n",
      "LOSS: 0.8556205034255981\n",
      "LOSS: 1.3204970359802246\n",
      "LOSS: 0.7996090650558472\n",
      "LOSS: 0.18528977036476135\n",
      "LOSS: 1.3411072492599487\n",
      "LOSS: 1.2683542966842651\n",
      "LOSS: 0.6245033740997314\n",
      "LOSS: 0.6045987010002136\n",
      "LOSS: 0.8278214931488037\n",
      "LOSS: 0.8308961987495422\n",
      "LOSS: 0.8889721632003784\n",
      "LOSS: 1.3491190671920776\n",
      "LOSS: 0.7026505470275879\n",
      "LOSS: 1.7629863023757935\n",
      "LOSS: 1.463401198387146\n",
      "LOSS: 0.5110976696014404\n",
      "LOSS: 0.34841567277908325\n",
      "LOSS: 0.5939971804618835\n",
      "LOSS: 1.0926597118377686\n",
      "LOSS: 0.5031405687332153\n",
      "LOSS: 0.5916845798492432\n",
      "LOSS: 1.345118761062622\n",
      "LOSS: 0.5940314531326294\n",
      "LOSS: 1.4051103591918945\n",
      "LOSS: 0.664126992225647\n",
      "LOSS: 0.5503801703453064\n",
      "LOSS: 0.7295571565628052\n",
      "LOSS: 0.5158624649047852\n",
      "LOSS: 1.0418262481689453\n",
      "LOSS: 1.0005815029144287\n",
      "LOSS: 1.0228976011276245\n",
      "LOSS: 0.6765744686126709\n",
      "LOSS: 1.1170413494110107\n",
      "LOSS: 0.8229949474334717\n",
      "LOSS: 0.9346952438354492\n",
      "LOSS: 1.143532395362854\n",
      "LOSS: 1.250609278678894\n",
      "LOSS: 0.8141092658042908\n",
      "LOSS: 1.4511682987213135\n",
      "LOSS: 1.2348359823226929\n",
      "LOSS: 0.9246337413787842\n",
      "LOSS: 0.9395435452461243\n",
      "LOSS: 0.4417135715484619\n",
      "LOSS: 1.3457777500152588\n",
      "LOSS: 1.0308353900909424\n",
      "LOSS: 1.2883508205413818\n",
      "LOSS: 0.7429870963096619\n",
      "LOSS: 1.493362545967102\n",
      "LOSS: 0.6402155160903931\n",
      "LOSS: 0.34243667125701904\n",
      "LOSS: 0.9735171794891357\n",
      "LOSS: 1.5291814804077148\n",
      "LOSS: 0.905768871307373\n",
      "LOSS: 0.6841951012611389\n",
      "LOSS: 1.1905124187469482\n",
      "LOSS: 0.679284930229187\n",
      "LOSS: 0.8887921571731567\n",
      "LOSS: 1.03136146068573\n",
      "LOSS: 0.7189661860466003\n",
      "LOSS: 1.073047399520874\n",
      "LOSS: 0.7177597880363464\n",
      "LOSS: 1.160356044769287\n",
      "LOSS: 1.165268063545227\n",
      "LOSS: 0.7593662142753601\n",
      "LOSS: 0.7358353734016418\n",
      "LOSS: 1.2970854043960571\n",
      "LOSS: 1.3219059705734253\n",
      "LOSS: 1.0541212558746338\n",
      "LOSS: 1.2510061264038086\n",
      "LOSS: 0.963459312915802\n",
      "LOSS: 1.1966300010681152\n",
      "LOSS: 0.9422895908355713\n",
      "LOSS: 0.9187209606170654\n",
      "LOSS: 0.8710079193115234\n",
      "LOSS: 0.9549583196640015\n",
      "LOSS: 2.2012696266174316\n",
      "LOSS: 1.3498308658599854\n",
      "LOSS: 0.697447657585144\n",
      "LOSS: 1.639430284500122\n",
      "LOSS: 0.7086029052734375\n",
      "LOSS: 0.6784806847572327\n",
      "LOSS: 0.6495019793510437\n",
      "LOSS: 0.8883687257766724\n",
      "LOSS: 0.36132803559303284\n",
      "LOSS: 0.475816547870636\n",
      "LOSS: 1.0545324087142944\n",
      "LOSS: 0.7018945217132568\n",
      "LOSS: 0.7047934532165527\n",
      "LOSS: 1.4578075408935547\n",
      "LOSS: 1.2958638668060303\n",
      "LOSS: 1.1461985111236572\n",
      "LOSS: 0.8702118992805481\n",
      "LOSS: 0.770002543926239\n",
      "LOSS: 0.6042689681053162\n",
      "LOSS: 1.2147897481918335\n",
      "LOSS: 0.9280184507369995\n",
      "LOSS: 0.7599613666534424\n",
      "LOSS: 0.8964290618896484\n",
      "LOSS: 1.016255259513855\n",
      "LOSS: 0.6299775242805481\n",
      "LOSS: 0.8883622288703918\n",
      "LOSS: 0.6994771361351013\n",
      "LOSS: 0.9277969002723694\n",
      "LOSS: 1.584984302520752\n",
      "LOSS: 1.229962944984436\n",
      "LOSS: 1.288661003112793\n",
      "LOSS: 0.6527791023254395\n",
      "LOSS: 0.8628085255622864\n",
      "LOSS: 0.9151580333709717\n",
      "LOSS: 1.237155556678772\n",
      "LOSS: 1.5498806238174438\n",
      "LOSS: 1.0397793054580688\n",
      "LOSS: 0.6590214371681213\n",
      "LOSS: 0.7886945009231567\n",
      "LOSS: 0.8524144887924194\n",
      "LOSS: 1.087526798248291\n",
      "LOSS: 1.26248300075531\n",
      "LOSS: 0.8724631071090698\n",
      "LOSS: 0.7024745345115662\n",
      "LOSS: 0.6819226145744324\n",
      "LOSS: 1.1458780765533447\n",
      "LOSS: 1.034293532371521\n",
      "LOSS: 0.24515555799007416\n",
      "LOSS: 0.5445480346679688\n",
      "LOSS: 0.9002448916435242\n",
      "LOSS: 1.6959704160690308\n",
      "LOSS: 0.2376323938369751\n",
      "LOSS: 0.6845592856407166\n",
      "LOSS: 0.9823434948921204\n",
      "LOSS: 0.7116193771362305\n",
      "LOSS: 1.0010133981704712\n",
      "LOSS: 0.7889528274536133\n",
      "LOSS: 0.528937816619873\n",
      "LOSS: 1.204755425453186\n",
      "LOSS: 1.0188156366348267\n",
      "LOSS: 0.8883811235427856\n",
      "LOSS: 1.0389968156814575\n",
      "LOSS: 1.2749594449996948\n",
      "LOSS: 0.37740829586982727\n",
      "LOSS: 0.6933664083480835\n",
      "LOSS: 1.0358929634094238\n",
      "LOSS: 0.8429459929466248\n",
      "LOSS: 0.791609525680542\n",
      "LOSS: 1.2260215282440186\n",
      "LOSS: 1.1281126737594604\n",
      "LOSS: 0.6698727607727051\n",
      "LOSS: 1.184246301651001\n",
      "LOSS: 1.2867432832717896\n",
      "LOSS: 1.1395702362060547\n",
      "LOSS: 0.9366186857223511\n",
      "LOSS: 1.080180287361145\n",
      "LOSS: 1.375589370727539\n",
      "LOSS: 1.2391552925109863\n",
      "LOSS: 1.4412156343460083\n",
      "LOSS: 0.9725247621536255\n",
      "LOSS: 0.9434100985527039\n",
      "LOSS: 0.94815993309021\n",
      "LOSS: 0.6180974841117859\n",
      "LOSS: 0.5894909501075745\n",
      "LOSS: 1.3770735263824463\n",
      "LOSS: 0.7179575562477112\n",
      "LOSS: 0.5194249153137207\n",
      "LOSS: 0.9307411313056946\n",
      "LOSS: 0.8948209881782532\n",
      "LOSS: 0.36426639556884766\n",
      "LOSS: 1.0135688781738281\n",
      "LOSS: 1.3156465291976929\n",
      "LOSS: 1.704158902168274\n",
      "LOSS: 0.7380836009979248\n",
      "LOSS: 1.1130566596984863\n",
      "LOSS: 0.3567837178707123\n",
      "LOSS: 1.326647162437439\n",
      "LOSS: 0.8847476840019226\n",
      "LOSS: 1.3501389026641846\n",
      "LOSS: 0.8846463561058044\n",
      "LOSS: 1.615665078163147\n",
      "LOSS: 1.0322325229644775\n",
      "LOSS: 0.8614739775657654\n",
      "LOSS: 0.49816957116127014\n",
      "LOSS: 0.7550516724586487\n",
      "LOSS: 1.2759852409362793\n",
      "LOSS: 0.991217851638794\n",
      "LOSS: 1.260573148727417\n",
      "LOSS: 1.237613558769226\n",
      "LOSS: 1.5524020195007324\n",
      "LOSS: 1.5696420669555664\n",
      "LOSS: 1.1258236169815063\n",
      "LOSS: 0.4744393527507782\n",
      "LOSS: 1.4005533456802368\n",
      "LOSS: 1.2984336614608765\n",
      "LOSS: 0.9592618942260742\n",
      "LOSS: 0.6020996570587158\n",
      "LOSS: 0.9686272144317627\n",
      "LOSS: 0.9722063541412354\n",
      "LOSS: 1.3008232116699219\n",
      "LOSS: 0.7406005859375\n",
      "LOSS: 1.4163014888763428\n",
      "LOSS: 0.8094337582588196\n",
      "LOSS: 1.0971065759658813\n",
      "LOSS: 1.2865550518035889\n",
      "LOSS: 0.6478098034858704\n",
      "LOSS: 0.3753306567668915\n",
      "LOSS: 0.4493197202682495\n",
      "LOSS: 0.7777652144432068\n",
      "LOSS: 1.4647883176803589\n",
      "LOSS: 0.6083062291145325\n",
      "LOSS: 0.83977210521698\n",
      "LOSS: 0.7950369119644165\n",
      "LOSS: 0.7430837750434875\n",
      "LOSS: 0.7976149320602417\n",
      "LOSS: 0.4583018720149994\n",
      "LOSS: 0.8424032330513\n",
      "LOSS: 0.6755356788635254\n",
      "LOSS: 1.3302267789840698\n",
      "LOSS: 0.895409882068634\n",
      "LOSS: 0.4345146119594574\n",
      "LOSS: 0.8982690572738647\n",
      "LOSS: 0.5518357157707214\n",
      "LOSS: 0.5824333429336548\n",
      "LOSS: 1.2868043184280396\n",
      "LOSS: 0.795439600944519\n",
      "LOSS: 0.41683074831962585\n",
      "LOSS: 1.2965306043624878\n",
      "LOSS: 1.3276963233947754\n",
      "LOSS: 1.0253074169158936\n",
      "LOSS: 1.0087345838546753\n",
      "LOSS: 1.872780680656433\n",
      "LOSS: 1.6686729192733765\n",
      "LOSS: 1.107990026473999\n",
      "LOSS: 0.46788230538368225\n",
      "LOSS: 1.1660159826278687\n",
      "LOSS: 1.3452982902526855\n",
      "LOSS: 0.8936786651611328\n",
      "LOSS: 0.7357368469238281\n",
      "LOSS: 1.081964135169983\n",
      "LOSS: 1.443141222000122\n",
      "LOSS: 1.063792109489441\n",
      "LOSS: 2.115055561065674\n",
      "LOSS: 1.0550565719604492\n",
      "LOSS: 0.7570958733558655\n",
      "LOSS: 0.43269166350364685\n",
      "LOSS: 1.0605937242507935\n",
      "LOSS: 1.214003324508667\n",
      "LOSS: 0.7306362986564636\n",
      "LOSS: 1.2760306596755981\n",
      "LOSS: 0.6351446509361267\n",
      "LOSS: 0.9691155552864075\n",
      "LOSS: 1.112073302268982\n",
      "LOSS: 1.214448094367981\n",
      "LOSS: 0.80507892370224\n",
      "LOSS: 0.7686576843261719\n",
      "LOSS: 0.9192479252815247\n",
      "LOSS: 0.567396879196167\n",
      "LOSS: 0.8972750902175903\n",
      "LOSS: 1.0504212379455566\n",
      "LOSS: 1.1364353895187378\n",
      "LOSS: 1.229174017906189\n",
      "LOSS: 1.096848487854004\n",
      "LOSS: 1.1195755004882812\n",
      "LOSS: 1.3838069438934326\n",
      "LOSS: 0.8747298717498779\n",
      "LOSS: 0.9737043976783752\n",
      "LOSS: 0.9705137014389038\n",
      "LOSS: 0.38958653807640076\n",
      "LOSS: 0.8306657075881958\n",
      "LOSS: 0.7758578658103943\n",
      "LOSS: 0.6881248354911804\n",
      "LOSS: 1.3672685623168945\n",
      "LOSS: 0.6876155138015747\n",
      "LOSS: 1.3115756511688232\n",
      "LOSS: 1.301253080368042\n",
      "LOSS: 1.7621933221817017\n",
      "LOSS: 0.8024640679359436\n",
      "LOSS: 1.0931010246276855\n",
      "LOSS: 1.7741981744766235\n",
      "LOSS: 0.8889600038528442\n",
      "LOSS: 1.1039354801177979\n",
      "LOSS: 1.2464351654052734\n",
      "LOSS: 0.45020991563796997\n",
      "LOSS: 1.4467902183532715\n",
      "LOSS: 0.3765077292919159\n",
      "LOSS: 0.5120264291763306\n",
      "LOSS: 1.6971638202667236\n",
      "LOSS: 1.0582995414733887\n",
      "LOSS: 4.279590129852295\n",
      "LOSS: 0.8224351406097412\n",
      "LOSS: 0.7867318391799927\n",
      "LOSS: 0.8702364563941956\n",
      "LOSS: 0.778342068195343\n",
      "LOSS: 0.45424404740333557\n",
      "LOSS: 0.7377506494522095\n",
      "LOSS: 1.502880573272705\n",
      "LOSS: 0.8741090893745422\n",
      "LOSS: 0.5278144478797913\n",
      "LOSS: 1.5681525468826294\n",
      "LOSS: 0.8450150489807129\n",
      "LOSS: 0.516150176525116\n",
      "LOSS: 1.080575704574585\n",
      "LOSS: 0.9733662605285645\n",
      "LOSS: 1.0377895832061768\n",
      "LOSS: 1.2367240190505981\n",
      "LOSS: 1.0132403373718262\n",
      "LOSS: 1.0057933330535889\n",
      "LOSS: 0.6381304264068604\n",
      "LOSS: 0.735717236995697\n",
      "LOSS: 0.9544814229011536\n",
      "LOSS: 0.7908503413200378\n",
      "LOSS: 0.6909700036048889\n",
      "LOSS: 0.5753598809242249\n",
      "LOSS: 0.7063844799995422\n",
      "LOSS: 0.9376099109649658\n",
      "LOSS: 2.0476598739624023\n",
      "LOSS: 1.2353638410568237\n",
      "LOSS: 0.7785224914550781\n",
      "LOSS: 1.1902811527252197\n",
      "LOSS: 0.5721465945243835\n",
      "LOSS: 0.6524622440338135\n",
      "LOSS: 1.2471920251846313\n",
      "LOSS: 1.1499686241149902\n",
      "LOSS: 1.1101468801498413\n",
      "LOSS: 0.9574967622756958\n",
      "LOSS: 1.3257322311401367\n",
      "LOSS: 0.32368844747543335\n",
      "LOSS: 1.1983119249343872\n",
      "LOSS: 1.4995161294937134\n",
      "LOSS: 0.7790931463241577\n",
      "LOSS: 0.7394543290138245\n",
      "LOSS: 0.9879165887832642\n",
      "LOSS: 1.0120512247085571\n",
      "LOSS: 0.9043673276901245\n",
      "LOSS: 0.673917293548584\n",
      "LOSS: 0.5336223244667053\n",
      "LOSS: 0.8172236084938049\n",
      "LOSS: 1.238246202468872\n",
      "LOSS: 1.5442712306976318\n",
      "LOSS: 0.9294662475585938\n",
      "LOSS: 0.7179484367370605\n",
      "LOSS: 0.5587640404701233\n",
      "LOSS: 1.0798240900039673\n",
      "LOSS: 0.7256644368171692\n",
      "LOSS: 0.6881375908851624\n",
      "LOSS: 1.1825015544891357\n",
      "LOSS: 0.8554695844650269\n",
      "LOSS: 0.7109162211418152\n",
      "LOSS: 1.1326823234558105\n",
      "LOSS: 0.9366917610168457\n",
      "LOSS: 1.4785735607147217\n",
      "LOSS: 0.5981622934341431\n",
      "LOSS: 1.2466870546340942\n",
      "LOSS: 1.1904172897338867\n",
      "LOSS: 0.3931090533733368\n",
      "LOSS: 1.4264566898345947\n",
      "LOSS: 1.3217642307281494\n",
      "LOSS: 1.2465527057647705\n",
      "LOSS: 0.8141891956329346\n",
      "LOSS: 0.6206261515617371\n",
      "LOSS: 1.2895441055297852\n",
      "LOSS: 0.7948302626609802\n",
      "LOSS: 1.3163065910339355\n",
      "LOSS: 0.5296823978424072\n",
      "LOSS: 1.9173760414123535\n",
      "LOSS: 1.3383522033691406\n",
      "LOSS: 0.7594989538192749\n",
      "LOSS: 0.633363664150238\n",
      "LOSS: 0.5085384845733643\n",
      "LOSS: 0.5574492812156677\n",
      "LOSS: 1.0862895250320435\n",
      "LOSS: 1.169919729232788\n",
      "LOSS: 1.4615530967712402\n",
      "LOSS: 0.899774432182312\n",
      "LOSS: 0.8182405233383179\n",
      "LOSS: 1.4066029787063599\n",
      "LOSS: 2.0365872383117676\n",
      "LOSS: 1.1102499961853027\n",
      "LOSS: 1.0906685590744019\n",
      "LOSS: 0.6346523761749268\n",
      "LOSS: 1.9294930696487427\n",
      "LOSS: 1.2640876770019531\n",
      "LOSS: 0.5164635181427002\n",
      "LOSS: 0.9159018397331238\n",
      "LOSS: 0.9227333664894104\n",
      "LOSS: 0.9004260301589966\n",
      "LOSS: 1.195375919342041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.46      0.47       382\n",
      "           1       0.53      0.55      0.54       429\n",
      "\n",
      "    accuracy                           0.51       811\n",
      "   macro avg       0.51      0.51      0.51       811\n",
      "weighted avg       0.51      0.51      0.51       811\n",
      "\n",
      "Balanced Accuracy Score: 0.5052813678468129\n",
      "Accuracy Score: 0.5080147965474723\n"
     ]
    }
   ],
   "source": [
    "def generate_predictions(model,df_test):\n",
    "    sentences = df_test.text.tolist()\n",
    "    batch_size = 32  \n",
    "    all_outputs = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", \n",
    "        padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "        for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "        \n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "\n",
    "generate_predictions(model,test_df)\n",
    "get_metrics_result(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.51      0.50      3102\n",
      "           1       0.54      0.52      0.53      3382\n",
      "\n",
      "    accuracy                           0.52      6484\n",
      "   macro avg       0.52      0.52      0.52      6484\n",
      "weighted avg       0.52      0.52      0.52      6484\n",
      "\n",
      "Balanced Accuracy Score: 0.5176166842246337\n",
      "Accuracy Score: 0.5178901912399754\n"
     ]
    }
   ],
   "source": [
    "generate_predictions(model,train_df)\n",
    "get_metrics_result(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original post: A multitude of films have twist...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Original post: I think the real reason people ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Original post: I think the real reason people ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Original post: While I agree with a lot of wha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Original post: While I agree with a lot of wha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Original post: While I agree with a lot of wha...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Original post: While I agree with a lot of wha...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Original post: Disclaimer: I am in no way just...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Original post: Disclaimer: I am in no way just...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Original post: Disclaimer: I am in no way just...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Original post: Disclaimer: I am in no way just...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Original post: Disclaimer: I am in no way just...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Original post: I've been trying to figure out ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Original post: I've been trying to figure out ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Original post: I believe that people have a **...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Original post: I believe that people have a **...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Original post: Cabotage is the practice of all...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Original post: Cabotage is the practice of all...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Original post: This may be surprising to some ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Original post: This may be surprising to some ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label  predictions\n",
       "0   Original post: A multitude of films have twist...      0            0\n",
       "1   Original post: I think the real reason people ...      1            0\n",
       "2   Original post: I think the real reason people ...      0            0\n",
       "3   Original post: While I agree with a lot of wha...      1            1\n",
       "4   Original post: While I agree with a lot of wha...      1            1\n",
       "5   Original post: While I agree with a lot of wha...      0            1\n",
       "6   Original post: While I agree with a lot of wha...      0            1\n",
       "7   Original post: Disclaimer: I am in no way just...      1            1\n",
       "8   Original post: Disclaimer: I am in no way just...      1            1\n",
       "9   Original post: Disclaimer: I am in no way just...      1            0\n",
       "10  Original post: Disclaimer: I am in no way just...      0            0\n",
       "11  Original post: Disclaimer: I am in no way just...      0            1\n",
       "12  Original post: I've been trying to figure out ...      1            1\n",
       "13  Original post: I've been trying to figure out ...      0            0\n",
       "14  Original post: I believe that people have a **...      1            1\n",
       "15  Original post: I believe that people have a **...      0            1\n",
       "16  Original post: Cabotage is the practice of all...      1            0\n",
       "17  Original post: Cabotage is the practice of all...      0            1\n",
       "18  Original post: This may be surprising to some ...      1            0\n",
       "19  Original post: This may be surprising to some ...      1            0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43835976450649758ad2d4d004eb12fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/54.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/ArjunSohur/argument_classification/commit/1e29dded6e13686ab998931eaba5447a27a743c9', commit_message='Upload model', commit_description='', oid='1e29dded6e13686ab998931eaba5447a27a743c9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"ArjunSohur/argument_classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persuasion_env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
