{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2024/06/finetuning-llama-3-for-sequence-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q transformers accelerate trl bitsandbytes datasets evaluate\n",
    "# !pip install -q peft scikit-learn\n",
    "# !pip install -U \"huggingface_hub[cli]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../private_/hf_read_token\", \"r\") as f:\n",
    "  token = f.readline()\n",
    "\n",
    "hf_read = token\n",
    "\n",
    "with open(\"../private_/hf_write_token\", \"r\") as f:\n",
    "  token = f.readline()\n",
    "\n",
    "hf_write = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/arjunsohur/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token $hf_read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convokit import Corpus, download\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/arjunsohur/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# not sure if the r and w tokens are needed but oh well\n",
    "login(token=hf_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "corpus = Corpus(filename=download(\"winning-args-corpus\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of ids 293297\n",
      "8106\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 120\n",
      "    })\n",
      "    val: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 15\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "ids = corpus.get_utterance_ids()\n",
    "print(\"Len of ids\", len(ids))\n",
    "\n",
    "SPEAKER_BLACKLIST = ['DeltaBot','AutoModerator']\n",
    "training_trios = []\n",
    "\n",
    "for id in ids:\n",
    "  ut = corpus.get_utterance(id)\n",
    "  if ut.reply_to == ut.conversation_id and (ut.meta['success'] == 1 or ut.meta['success'] == 0) and (ut.speaker.id not in SPEAKER_BLACKLIST):\n",
    "    op = corpus.get_utterance(ut.conversation_id).text\n",
    "    x = ut.text\n",
    "    y = ut.meta['success']\n",
    "\n",
    "    training_trios += [(op, x, y)]\n",
    "\n",
    "print(len(training_trios))\n",
    "\n",
    "train_len = len(training_trios)\n",
    "\n",
    "ones = 0\n",
    "zeros = 0\n",
    "total = 0\n",
    "\n",
    "def formatting_prompts_func(training_trios):\n",
    "    texts = []\n",
    "    targets = []\n",
    "\n",
    "    total = 0\n",
    "    ones = 0\n",
    "    zeros = 0\n",
    "\n",
    "    for trio in training_trios:\n",
    "        op, x, y = trio\n",
    "        instruction = \"Please determine if the following argument is successful based on the original post.  Output 1 for successful and 0 for unsuccessful.  Only output the one number, NOTHING ELSE.\"\n",
    "        input_context = f\"Original post: {op}\\nArgument: {x}\"\n",
    "\n",
    "        texts.append(input_context)\n",
    "        targets.append(y)\n",
    "\n",
    "        if y:\n",
    "           ones+=1\n",
    "        else:\n",
    "           zeros+=1\n",
    "        total += 1\n",
    "\n",
    "    return texts, targets, ones, zeros, total\n",
    "\n",
    "# Format the data\n",
    "texts, targets, ones, zeros, total = formatting_prompts_func(training_trios)\n",
    "\n",
    "len_text = len(texts)\n",
    "len_text = int(len_text*(150/8106))\n",
    "\n",
    "v_start = int(len_text * 0.8)\n",
    "v_end = int(len_text * 0.9)\n",
    "\n",
    "train = {\"text\": texts[:v_start], \"label\": targets[:v_start]}\n",
    "val = {\"text\": texts[v_start:v_end], \"label\":targets[v_start:v_end]}\n",
    "test = {\"text\": texts[v_end:len_text], \"label\":targets[v_end:len_text]}\n",
    "\n",
    "train_ds = Dataset.from_dict(train)\n",
    "val_ds = Dataset.from_dict(val)\n",
    "test_ds = Dataset.from_dict(test)\n",
    "\n",
    "train_df = pd.DataFrame.from_dict(train_ds)\n",
    "val_df = pd.DataFrame.from_dict(val_ds)\n",
    "test_df = pd.DataFrame.from_dict(test_ds)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "   'train': train_ds,\n",
    "   'val': val_ds,\n",
    "   'test': test_ds\n",
    "})\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5167, 0.4833])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights=(1/train_df.label.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d0fde29cd44db49577593daddda8b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quantization_config,\n",
    "    num_labels=2,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    task_type = 'SEQ_CLS'\n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
    "\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = test_df.text.tolist()\n",
    "\n",
    "batch_size = 32  \n",
    "\n",
    "all_outputs = []\n",
    "\n",
    "for i in range(0, len(sentences), batch_size):\n",
    "    batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", \n",
    "    padding=True, truncation=True, max_length=512)\n",
    "\n",
    "    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        all_outputs.append(outputs['logits'])\n",
    "        \n",
    "final_outputs = torch.cat(all_outputs, dim=0)\n",
    "test_df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.14      0.20         7\n",
      "           1       0.50      0.75      0.60         8\n",
      "\n",
      "    accuracy                           0.47        15\n",
      "   macro avg       0.42      0.45      0.40        15\n",
      "weighted avg       0.42      0.47      0.41        15\n",
      "\n",
      "Balanced Accuracy Score: 0.4464285714285714\n",
      "Accuracy Score: 0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import balanced_accuracy_score, classification_report\n",
    "\n",
    "def get_metrics_result(test_df):\n",
    "    y_test = test_df.label\n",
    "    y_pred = test_df.predictions\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "get_metrics_result(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a4e59c8c26343989b584311726a0530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "501436e4459a489383254e41d5bab6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "807da4994f19457582b1126d93180417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def data_preprocesing(row):\n",
    "    return tokenizer(row['text'], truncation=True, max_length=1000)\n",
    "\n",
    "tokenized_data = dataset.map(data_preprocesing, batched=True, \n",
    "remove_columns=['text'])\n",
    "tokenized_data.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(evaluations):\n",
    "    predictions, labels = evaluations\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),\n",
    "    'accuracy':accuracy_score(predictions,labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "\n",
    "global train_loss\n",
    "train_loss = []\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, eval_dataset_train=None, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.eval_dataset_train = eval_dataset_train\n",
    "\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "    \n",
    "    def evaluate(self, eval_dataset=None, ignore_keys=None, metric_key_prefix=\"eval\"):\n",
    "        # Evaluate on validation set\n",
    "        eval_results = super().evaluate(eval_dataset, ignore_keys, metric_key_prefix)\n",
    "        \n",
    "        # Evaluate on training set\n",
    "        if self.eval_dataset_train is not None:\n",
    "            train_results = super().evaluate(self.eval_dataset_train, ignore_keys, metric_key_prefix=\"train\")\n",
    "\n",
    "            train_loss.append(train_results[\"train_loss\"])\n",
    "            \n",
    "            eval_results.update(train_results)\n",
    "        \n",
    "        print(f\"Mid-train results: Val loss: {eval_results[\"eval_loss\"]:.4f} || Train loss {eval_results[\"train_loss\"]:.4f}\")\n",
    "        \n",
    "        return eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='persuasion_classification',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    max_steps=2000,  # 500 steps * 4 \"epochs\" = 2000 total steps\n",
    "    logging_steps=10,\n",
    "    weight_decay=0.05,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=10,\n",
    "    save_steps=500,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",\n",
    "    lr_scheduler_type='linear',\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=500,\n",
    "    dataloader_drop_last=False,  # Ensure all data is used\n",
    "    remove_unused_columns=False  # Prevent potential issues with custom datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data['train'],\n",
    "    eval_dataset=tokenized_data['val'],\n",
    "    eval_dataset_train=tokenized_data['train'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1121' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1121/2000 2:43:45 < 2:08:38, 0.11 it/s, Epoch 18.67/34]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Balanced Accuracy</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.680100</td>\n",
       "      <td>1.812711</td>\n",
       "      <td>0.522708</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.587400</td>\n",
       "      <td>1.810749</td>\n",
       "      <td>0.522708</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.420900</td>\n",
       "      <td>1.808327</td>\n",
       "      <td>0.522708</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.051300</td>\n",
       "      <td>1.804466</td>\n",
       "      <td>0.522708</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.818100</td>\n",
       "      <td>1.799404</td>\n",
       "      <td>0.522708</td>\n",
       "      <td>0.525000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.296400</td>\n",
       "      <td>1.792659</td>\n",
       "      <td>0.531429</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.382200</td>\n",
       "      <td>1.783392</td>\n",
       "      <td>0.531429</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.716300</td>\n",
       "      <td>1.777586</td>\n",
       "      <td>0.531429</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.260700</td>\n",
       "      <td>1.758547</td>\n",
       "      <td>0.531429</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.288800</td>\n",
       "      <td>1.747182</td>\n",
       "      <td>0.540068</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>3.099000</td>\n",
       "      <td>1.736532</td>\n",
       "      <td>0.540068</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.945900</td>\n",
       "      <td>1.724766</td>\n",
       "      <td>0.540068</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.828600</td>\n",
       "      <td>1.710656</td>\n",
       "      <td>0.540068</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>1.771400</td>\n",
       "      <td>1.690637</td>\n",
       "      <td>0.531674</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.714000</td>\n",
       "      <td>1.667749</td>\n",
       "      <td>0.531987</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.295200</td>\n",
       "      <td>1.647668</td>\n",
       "      <td>0.540559</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.467600</td>\n",
       "      <td>1.628987</td>\n",
       "      <td>0.540559</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>1.181500</td>\n",
       "      <td>1.608309</td>\n",
       "      <td>0.540559</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>1.564200</td>\n",
       "      <td>1.585621</td>\n",
       "      <td>0.548822</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.978200</td>\n",
       "      <td>1.561726</td>\n",
       "      <td>0.557343</td>\n",
       "      <td>0.558333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>1.849700</td>\n",
       "      <td>1.555129</td>\n",
       "      <td>0.531674</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>1.123200</td>\n",
       "      <td>1.540573</td>\n",
       "      <td>0.540270</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.292200</td>\n",
       "      <td>1.506215</td>\n",
       "      <td>0.548822</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>1.717500</td>\n",
       "      <td>1.472232</td>\n",
       "      <td>0.548822</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.070300</td>\n",
       "      <td>1.434191</td>\n",
       "      <td>0.557343</td>\n",
       "      <td>0.558333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>1.919200</td>\n",
       "      <td>1.395659</td>\n",
       "      <td>0.557343</td>\n",
       "      <td>0.558333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>1.107400</td>\n",
       "      <td>1.357200</td>\n",
       "      <td>0.557644</td>\n",
       "      <td>0.558333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>1.367700</td>\n",
       "      <td>1.325116</td>\n",
       "      <td>0.565657</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>1.128900</td>\n",
       "      <td>1.278081</td>\n",
       "      <td>0.557343</td>\n",
       "      <td>0.558333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.988500</td>\n",
       "      <td>1.215269</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>0.591667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.114700</td>\n",
       "      <td>1.146611</td>\n",
       "      <td>0.599330</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.581900</td>\n",
       "      <td>1.110205</td>\n",
       "      <td>0.625320</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.446500</td>\n",
       "      <td>1.061124</td>\n",
       "      <td>0.642370</td>\n",
       "      <td>0.641667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>1.278800</td>\n",
       "      <td>0.971175</td>\n",
       "      <td>0.633484</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.858100</td>\n",
       "      <td>0.919767</td>\n",
       "      <td>0.633484</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.781500</td>\n",
       "      <td>0.894393</td>\n",
       "      <td>0.632925</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.702400</td>\n",
       "      <td>0.893063</td>\n",
       "      <td>0.670034</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.948800</td>\n",
       "      <td>0.874572</td>\n",
       "      <td>0.684248</td>\n",
       "      <td>0.675000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.741300</td>\n",
       "      <td>0.931460</td>\n",
       "      <td>0.693750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.313800</td>\n",
       "      <td>0.782532</td>\n",
       "      <td>0.711806</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>1.196300</td>\n",
       "      <td>0.668449</td>\n",
       "      <td>0.726399</td>\n",
       "      <td>0.725000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.629400</td>\n",
       "      <td>0.629135</td>\n",
       "      <td>0.767269</td>\n",
       "      <td>0.758333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.526100</td>\n",
       "      <td>0.591014</td>\n",
       "      <td>0.771111</td>\n",
       "      <td>0.758333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.401000</td>\n",
       "      <td>0.534990</td>\n",
       "      <td>0.813161</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.463265</td>\n",
       "      <td>0.815895</td>\n",
       "      <td>0.808333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.639600</td>\n",
       "      <td>0.388151</td>\n",
       "      <td>0.864024</td>\n",
       "      <td>0.858333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.322434</td>\n",
       "      <td>0.850446</td>\n",
       "      <td>0.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.287700</td>\n",
       "      <td>0.306355</td>\n",
       "      <td>0.858396</td>\n",
       "      <td>0.858333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.322000</td>\n",
       "      <td>0.307392</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.258500</td>\n",
       "      <td>0.410066</td>\n",
       "      <td>0.839465</td>\n",
       "      <td>0.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.806600</td>\n",
       "      <td>0.285690</td>\n",
       "      <td>0.892053</td>\n",
       "      <td>0.891667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.468600</td>\n",
       "      <td>0.191614</td>\n",
       "      <td>0.943357</td>\n",
       "      <td>0.941667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.233800</td>\n",
       "      <td>0.266998</td>\n",
       "      <td>0.918919</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.343800</td>\n",
       "      <td>0.236389</td>\n",
       "      <td>0.936027</td>\n",
       "      <td>0.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.077800</td>\n",
       "      <td>0.154533</td>\n",
       "      <td>0.958183</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>0.152551</td>\n",
       "      <td>0.958183</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.041000</td>\n",
       "      <td>0.154217</td>\n",
       "      <td>0.958183</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.585300</td>\n",
       "      <td>0.155014</td>\n",
       "      <td>0.958647</td>\n",
       "      <td>0.958333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.104400</td>\n",
       "      <td>0.126819</td>\n",
       "      <td>0.966630</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.329400</td>\n",
       "      <td>0.111280</td>\n",
       "      <td>0.966630</td>\n",
       "      <td>0.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.079072</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.091400</td>\n",
       "      <td>0.081836</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.036400</td>\n",
       "      <td>0.088581</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.098569</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.001300</td>\n",
       "      <td>0.112498</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.133200</td>\n",
       "      <td>0.103677</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.095042</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.047000</td>\n",
       "      <td>0.097940</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.129474</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.197600</td>\n",
       "      <td>0.128396</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.118512</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.538200</td>\n",
       "      <td>0.090845</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.072611</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.073783</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.062600</td>\n",
       "      <td>0.097295</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.120440</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>770</td>\n",
       "      <td>0.752100</td>\n",
       "      <td>0.102140</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.079486</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>790</td>\n",
       "      <td>0.084400</td>\n",
       "      <td>0.070023</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.073408</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>0.069700</td>\n",
       "      <td>0.072436</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.079071</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>830</td>\n",
       "      <td>0.427200</td>\n",
       "      <td>0.080063</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.069665</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.068800</td>\n",
       "      <td>0.081876</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.082117</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.094977</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.104221</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>890</td>\n",
       "      <td>0.614200</td>\n",
       "      <td>0.069141</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.080335</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>910</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.101410</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>920</td>\n",
       "      <td>0.018800</td>\n",
       "      <td>0.123688</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>930</td>\n",
       "      <td>0.510800</td>\n",
       "      <td>0.143718</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>940</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.145346</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.146325</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>960</td>\n",
       "      <td>0.460200</td>\n",
       "      <td>0.117167</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>970</td>\n",
       "      <td>0.101800</td>\n",
       "      <td>0.091571</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>980</td>\n",
       "      <td>0.357100</td>\n",
       "      <td>0.074719</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>990</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.067772</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.076071</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1010</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.082960</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1020</td>\n",
       "      <td>0.130200</td>\n",
       "      <td>0.082987</td>\n",
       "      <td>0.974854</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1030</td>\n",
       "      <td>0.028900</td>\n",
       "      <td>0.091554</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1040</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.102628</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.117300</td>\n",
       "      <td>0.104567</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1060</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.094374</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1070</td>\n",
       "      <td>0.443500</td>\n",
       "      <td>0.080248</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1080</td>\n",
       "      <td>0.038200</td>\n",
       "      <td>0.070960</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1090</td>\n",
       "      <td>0.024300</td>\n",
       "      <td>0.074678</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.085756</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1110</td>\n",
       "      <td>0.111100</td>\n",
       "      <td>0.084916</td>\n",
       "      <td>0.976923</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1120</td>\n",
       "      <td>0.016100</td>\n",
       "      <td>1.323159</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.466667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='57' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [57/60 00:50 < 00:02, 1.12 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid-train results: Val loss: 1.2339 || Train loss 1.8127\n",
      "Mid-train results: Val loss: 1.2332 || Train loss 1.8107\n",
      "Mid-train results: Val loss: 1.2333 || Train loss 1.8083\n",
      "Mid-train results: Val loss: 1.2275 || Train loss 1.8045\n",
      "Mid-train results: Val loss: 1.2250 || Train loss 1.7994\n",
      "Mid-train results: Val loss: 1.2234 || Train loss 1.7927\n",
      "Mid-train results: Val loss: 1.2120 || Train loss 1.7834\n",
      "Mid-train results: Val loss: 1.2156 || Train loss 1.7776\n",
      "Mid-train results: Val loss: 1.1855 || Train loss 1.7585\n",
      "Mid-train results: Val loss: 1.1707 || Train loss 1.7472\n",
      "Mid-train results: Val loss: 1.1758 || Train loss 1.7365\n",
      "Mid-train results: Val loss: 1.1623 || Train loss 1.7248\n",
      "Mid-train results: Val loss: 1.1655 || Train loss 1.7107\n",
      "Mid-train results: Val loss: 1.1412 || Train loss 1.6906\n",
      "Mid-train results: Val loss: 1.1079 || Train loss 1.6677\n",
      "Mid-train results: Val loss: 1.0776 || Train loss 1.6477\n",
      "Mid-train results: Val loss: 1.0728 || Train loss 1.6290\n",
      "Mid-train results: Val loss: 1.0756 || Train loss 1.6083\n",
      "Mid-train results: Val loss: 1.0814 || Train loss 1.5856\n",
      "Mid-train results: Val loss: 1.0873 || Train loss 1.5617\n",
      "Mid-train results: Val loss: 1.1553 || Train loss 1.5551\n",
      "Mid-train results: Val loss: 1.1592 || Train loss 1.5406\n",
      "Mid-train results: Val loss: 1.1519 || Train loss 1.5062\n",
      "Mid-train results: Val loss: 1.1526 || Train loss 1.4722\n",
      "Mid-train results: Val loss: 1.1412 || Train loss 1.4342\n",
      "Mid-train results: Val loss: 1.0813 || Train loss 1.3957\n",
      "Mid-train results: Val loss: 1.0601 || Train loss 1.3572\n",
      "Mid-train results: Val loss: 1.0958 || Train loss 1.3251\n",
      "Mid-train results: Val loss: 1.0923 || Train loss 1.2781\n",
      "Mid-train results: Val loss: 1.1020 || Train loss 1.2153\n",
      "Mid-train results: Val loss: 1.0999 || Train loss 1.1466\n",
      "Mid-train results: Val loss: 1.1742 || Train loss 1.1102\n",
      "Mid-train results: Val loss: 1.1909 || Train loss 1.0611\n",
      "Mid-train results: Val loss: 1.1466 || Train loss 0.9712\n",
      "Mid-train results: Val loss: 1.1437 || Train loss 0.9198\n",
      "Mid-train results: Val loss: 1.0805 || Train loss 0.8944\n",
      "Mid-train results: Val loss: 1.0421 || Train loss 0.8931\n",
      "Mid-train results: Val loss: 1.0323 || Train loss 0.8746\n",
      "Mid-train results: Val loss: 1.1017 || Train loss 0.9315\n",
      "Mid-train results: Val loss: 1.0559 || Train loss 0.7825\n",
      "Mid-train results: Val loss: 1.1403 || Train loss 0.6684\n",
      "Mid-train results: Val loss: 1.3167 || Train loss 0.6291\n",
      "Mid-train results: Val loss: 1.4003 || Train loss 0.5910\n",
      "Mid-train results: Val loss: 1.3934 || Train loss 0.5350\n",
      "Mid-train results: Val loss: 1.3418 || Train loss 0.4633\n",
      "Mid-train results: Val loss: 1.2692 || Train loss 0.3882\n",
      "Mid-train results: Val loss: 1.2438 || Train loss 0.3224\n",
      "Mid-train results: Val loss: 1.1903 || Train loss 0.3064\n",
      "Mid-train results: Val loss: 1.1713 || Train loss 0.3074\n",
      "Mid-train results: Val loss: 1.2101 || Train loss 0.4101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid-train results: Val loss: 1.1390 || Train loss 0.2857\n",
      "Mid-train results: Val loss: 1.2244 || Train loss 0.1916\n",
      "Mid-train results: Val loss: 1.8728 || Train loss 0.2670\n",
      "Mid-train results: Val loss: 1.8503 || Train loss 0.2364\n",
      "Mid-train results: Val loss: 1.3278 || Train loss 0.1545\n",
      "Mid-train results: Val loss: 1.1285 || Train loss 0.1526\n",
      "Mid-train results: Val loss: 1.2537 || Train loss 0.1542\n",
      "Mid-train results: Val loss: 1.3497 || Train loss 0.1550\n",
      "Mid-train results: Val loss: 1.3604 || Train loss 0.1268\n",
      "Mid-train results: Val loss: 1.3433 || Train loss 0.1113\n",
      "Mid-train results: Val loss: 1.1741 || Train loss 0.0791\n",
      "Mid-train results: Val loss: 1.1548 || Train loss 0.0818\n",
      "Mid-train results: Val loss: 1.3230 || Train loss 0.0886\n",
      "Mid-train results: Val loss: 1.4300 || Train loss 0.0986\n",
      "Mid-train results: Val loss: 1.5406 || Train loss 0.1125\n",
      "Mid-train results: Val loss: 1.4481 || Train loss 0.1037\n",
      "Mid-train results: Val loss: 1.3951 || Train loss 0.0950\n",
      "Mid-train results: Val loss: 1.4575 || Train loss 0.0979\n",
      "Mid-train results: Val loss: 1.5211 || Train loss 0.1295\n",
      "Mid-train results: Val loss: 1.3712 || Train loss 0.1284\n",
      "Mid-train results: Val loss: 1.1957 || Train loss 0.1185\n",
      "Mid-train results: Val loss: 1.1041 || Train loss 0.0908\n",
      "Mid-train results: Val loss: 1.1633 || Train loss 0.0726\n",
      "Mid-train results: Val loss: 1.2907 || Train loss 0.0738\n",
      "Mid-train results: Val loss: 1.3274 || Train loss 0.0973\n",
      "Mid-train results: Val loss: 1.3089 || Train loss 0.1204\n",
      "Mid-train results: Val loss: 1.3185 || Train loss 0.1021\n",
      "Mid-train results: Val loss: 1.3469 || Train loss 0.0795\n",
      "Mid-train results: Val loss: 1.3788 || Train loss 0.0700\n",
      "Mid-train results: Val loss: 1.3727 || Train loss 0.0734\n",
      "Mid-train results: Val loss: 1.3301 || Train loss 0.0724\n",
      "Mid-train results: Val loss: 1.3101 || Train loss 0.0791\n",
      "Mid-train results: Val loss: 1.2914 || Train loss 0.0801\n",
      "Mid-train results: Val loss: 1.2552 || Train loss 0.0697\n",
      "Mid-train results: Val loss: 1.2996 || Train loss 0.0819\n",
      "Mid-train results: Val loss: 1.2969 || Train loss 0.0821\n",
      "Mid-train results: Val loss: 1.2870 || Train loss 0.0950\n",
      "Mid-train results: Val loss: 1.2767 || Train loss 0.1042\n",
      "Mid-train results: Val loss: 1.2494 || Train loss 0.0691\n",
      "Mid-train results: Val loss: 1.3121 || Train loss 0.0803\n",
      "Mid-train results: Val loss: 1.3919 || Train loss 0.1014\n",
      "Mid-train results: Val loss: 1.4292 || Train loss 0.1237\n",
      "Mid-train results: Val loss: 1.3944 || Train loss 0.1437\n",
      "Mid-train results: Val loss: 1.3419 || Train loss 0.1453\n",
      "Mid-train results: Val loss: 1.3252 || Train loss 0.1463\n",
      "Mid-train results: Val loss: 1.3007 || Train loss 0.1172\n",
      "Mid-train results: Val loss: 1.2807 || Train loss 0.0916\n",
      "Mid-train results: Val loss: 1.2287 || Train loss 0.0747\n",
      "Mid-train results: Val loss: 1.1994 || Train loss 0.0678\n",
      "Mid-train results: Val loss: 1.2346 || Train loss 0.0761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arjunsohur/miniconda3/envs/persuasion_env3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mid-train results: Val loss: 1.2686 || Train loss 0.0830\n",
      "Mid-train results: Val loss: 1.3045 || Train loss 0.0830\n",
      "Mid-train results: Val loss: 1.3376 || Train loss 0.0916\n",
      "Mid-train results: Val loss: 1.3572 || Train loss 0.1026\n",
      "Mid-train results: Val loss: 1.3454 || Train loss 0.1046\n",
      "Mid-train results: Val loss: 1.3299 || Train loss 0.0944\n",
      "Mid-train results: Val loss: 1.2817 || Train loss 0.0802\n",
      "Mid-train results: Val loss: 1.2459 || Train loss 0.0710\n",
      "Mid-train results: Val loss: 1.2568 || Train loss 0.0747\n",
      "Mid-train results: Val loss: 1.2864 || Train loss 0.0858\n",
      "Mid-train results: Val loss: 1.3127 || Train loss 0.0849\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.53      0.49        30\n",
      "           1       0.50      0.42      0.46        33\n",
      "\n",
      "    accuracy                           0.48        63\n",
      "   macro avg       0.48      0.48      0.48        63\n",
      "weighted avg       0.48      0.48      0.47        63\n",
      "\n",
      "Balanced Accuracy Score: 0.47878787878787876\n",
      "Accuracy Score: 0.47619047619047616\n"
     ]
    }
   ],
   "source": [
    "def generate_predictions(model,df_test):\n",
    "    sentences = df_test.text.tolist()\n",
    "    batch_size = 32  \n",
    "    all_outputs = []\n",
    "\n",
    "    for i in range(0, len(sentences), batch_size):\n",
    "\n",
    "        batch_sentences = sentences[i:i + batch_size]\n",
    "\n",
    "        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", \n",
    "        padding=True, truncation=True, max_length=512)\n",
    "\n",
    "        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "        for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            all_outputs.append(outputs['logits'])\n",
    "        \n",
    "    final_outputs = torch.cat(all_outputs, dim=0)\n",
    "    df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
    "\n",
    "generate_predictions(model,test_df)\n",
    "get_metrics_result(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.50      0.51       237\n",
      "           1       0.56      0.57      0.57       263\n",
      "\n",
      "    accuracy                           0.54       500\n",
      "   macro avg       0.54      0.54      0.54       500\n",
      "weighted avg       0.54      0.54      0.54       500\n",
      "\n",
      "Balanced Accuracy Score: 0.5360173910253325\n",
      "Accuracy Score: 0.538\n"
     ]
    }
   ],
   "source": [
    "generate_predictions(model,train_df)\n",
    "get_metrics_result(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"ArjunSohur/argument_classification\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persuasion_env4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
